#!/bin/bash -l

## SLURM job submission script for running the encoder-decoder neural network script (unet.py)
##
## Request a single compute node containing 1 Pascal architecture GPU

#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --constraint=p100

## Specify the SLURM partition to be used and a miximum runtime of 1 hour

#SBATCH --time=00:10:00
#SBATCH --partition=gpuq-dev

## Give a name to the run and do not copy the login node environment settings onto the compute node

#SBATCH --job-name=ChannelCorrelation
#SBATCH --account=pawsey0001
#SBATCH --export=MYSCRATCH

# Set the name of the python script to be ran and the container image we will load/run

ML_SCRIPT=basic_chan.py
CONTAINER=chainer/chainer:latest-python3

# Set values to the configurable hyperparameters in the ML script

EPOCHS=1
BATCH_SIZE=128
LEARN_RATE=0.001
CHANNELS=1

##
##---- Do not change anything below this line ----
##

module load shifter 

export CUPY_CACHE_DIR=$MYSCRATCH/cupy
export CUPY_CACHE_SAVE_CUDA_SOURCE=0
export CUPY_DUMP_CUDA_SOURCE_ON_ERROR=0

#srun -n 1 --export=ALL shifter run ${CONTAINER} python3 ${ML_SCRIPT} -l ${LEARN_RATE} -g 1 -e ${EPOCHS} -b ${BATCH_SIZE} -c ${CHANNELS}
srun -n 1 --export=ALL shifter run ${CONTAINER} python3 train_cifar.py

#!/bin/bash -l

## SLURM job submission script for running the encoder-decoder neural network script (unet.py)
##
## Request two compute nodes containing 1 Pascal architecture GPU each

#SBATCH --nodes=2
#SBATCH --gres=gpu:1
#SBATCH --constraint=p100

## Specify the SLURM partition to be used and a miximum runtime of 1 hour

#SBATCH --time=00:20:00
#SBATCH --partition=gpuq-dev

## Give a run to the run and do not copy the login node environment settings onto the compute node

#SBATCH --job-name=weather
#SBATCH --account=pawsey0001
#SBATCH --export=NONE

# Set the name of the python script to be ran and the container image we will load/run

ML_SCRIPT=unet.py
CONTAINER=uber/horovod:0.15.1-tf1.11.0-torch0.4.1-py3.5

# Set values to the configurable hyperparameters in the ML script

EPOCHS=4
BATCH_SIZE=64
LEARN_RATE=0.001
L2=0.00001
MIN_DELTA=0.01

##
##---- Do not change anything below this line ----
##

module unload gcc/4.8.5 sandybridge
module load broadwell gcc/5.5.0 shifter cuda/9.2 openmpi/3.1.3
mpirun -np 2 -bind-to none -map-by slot -mca pml ob1 -mca btl_openib_receive_queues P,128,32:P,2048,32:P,12288,32:P,65536,32 -x NCCL_IB_CUDA_SUPPORT=1 -x NCCL_IB_DISABLE=0 -x NCCL_DEBUG=INFO shifter run ${CONTAINER}  python ${ML_SCRIPT} -e ${EPOCHS} -b ${BATCH_SIZE} -l ${LEARN_RATE} -r ${L2} -m ${MIN_DELTA} &
MPI_PID=$!

wait $MPI_PID

